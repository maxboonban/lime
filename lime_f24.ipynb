{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxboonban/lime/blob/main/lime_f24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f2dea85-3b73-487f-a7d6-81e68b7747ae",
      "metadata": {
        "id": "9f2dea85-3b73-487f-a7d6-81e68b7747ae"
      },
      "source": [
        "# Mini Project 1: Model Interpretation with LIME\n",
        "---\n",
        "***Important***:\n",
        "- Before starting the lab please copy this notebook into your own google drive by clicking on \"File\" and \"Save a copy in drive\"\n",
        "- If you want to work locally, make sure that you also download all other files together to the same directory\n",
        "---\n",
        "\n",
        "In this lab, we will practice using the Local Interpretable Model-agnostic Explanations, or LIME, technique on neural networks.\n",
        "\n",
        "HINT:\n",
        "- Search \"Check-\" in browser search (cmd+f mac, ctrl+f windows)\n",
        "- You can also open the \"table of contents\" from the side menu on the left side of your screen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9d12d31-d7d8-4d3d-a762-a538d68f96b4",
      "metadata": {
        "id": "b9d12d31-d7d8-4d3d-a762-a538d68f96b4"
      },
      "source": [
        "Here are some boilerplate codes to make the notebook compatible with both Colab and local Jupyter environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebefe5e-7900-4fc5-8043-ed3ad26ad169",
      "metadata": {
        "id": "3ebefe5e-7900-4fc5-8043-ed3ad26ad169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "7a619e55-c37c-4d40-c6a5-aa91db8292cf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-082e9dbd25ae>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misColab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# colab_path = (\"/content/drive/Shared drives\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "isColab = \"google.colab\" in sys.modules\n",
        "# this also works:\n",
        "# isColab = \"COLAB_GPU\" in os.environ\n",
        "\n",
        "if isColab:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "    # colab_path = (\"/content/drive/Shared drives\"\n",
        "    #     + \"/CS1470 TAs Fall 2022/Labs/lab04_lime\")\n",
        "\n",
        "    # TODO: fill in path stencil below, and download appropriate files to your\n",
        "    # local colab dir\n",
        "    student_colab_path = (\"/content/drive/MyDrive\"\n",
        "        + \"/YOUR_PATH_TO_COPIED_COLAB_HERE\")\n",
        "    sys.path.append(student_colab_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d1f224-97ba-4845-bc7b-aebe7777a03c",
      "metadata": {
        "id": "78d1f224-97ba-4845-bc7b-aebe7777a03c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import json, itertools\n",
        "\n",
        "from sklearn.linear_model import Lasso # Lasso Regression for the glassbox model in LIME\n",
        "from skimage.color import rgb2gray # Convert the unimportant areas into grayscale\n",
        "from skimage.data import astronaut # example image\n",
        "from skimage.segmentation import quickshift, mark_boundaries # image segmentation algorithm\n",
        "\n",
        "# make sure that preprocess.py and limelab.py are either in the same folder as this notebook\n",
        "# or that their path is included in the system path variable.\n",
        "from preprocess import *\n",
        "import limelab\n",
        "\n",
        "# ensures that we run only on cpu\n",
        "# this environment variable is not permanent\n",
        "# it is valid only for this session\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c534d46b-7046-4003-bfd1-7a273cecd3e7",
      "metadata": {
        "id": "c534d46b-7046-4003-bfd1-7a273cecd3e7"
      },
      "outputs": [],
      "source": [
        "data_path = \"data\"   # CIFAR10 dataset\n",
        "model_path = \"model\" # Pretrained CNN model's JSON file and weight\n",
        "\n",
        "# If you are working on Colab, you need to modify the path your other files too.\n",
        "if isColab:\n",
        "    data_path = f\"{colab_path}/{data_path}\"\n",
        "    model_path = f\"{colab_path}/{model_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008c319a-3d01-488e-a5b8-109652718554",
      "metadata": {
        "id": "008c319a-3d01-488e-a5b8-109652718554"
      },
      "source": [
        "## Introduction to LIME"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0cd6284-622c-47d6-8b37-32445822cbdb",
      "metadata": {
        "id": "f0cd6284-622c-47d6-8b37-32445822cbdb"
      },
      "source": [
        "### Motivation and Conceptual Background"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e994db-2c04-4a7a-9659-ff0881a9c79c",
      "metadata": {
        "id": "61e994db-2c04-4a7a-9659-ff0881a9c79c"
      },
      "source": [
        "Deep learning models are capable of providing accurate predictions, but how do we know we can trust them? Deep learning models are \"blackboxes\" when they give us predictions or reach some decision without explaing or showing how they did so. In certain fields, such as medicine, it is important to know how the \"blackbox\" model created its predictions so we can trust they are accurate. The basic idea of [LIME (Local Interpretable Model-agnostic Explanations)](https://arxiv.org/abs/1602.04938?context=cs):\n",
        "- **Replace the \"blackbox\" model with the \"glassbox\"** model, which is much easier to interpret\n",
        "  - We can learn the behavior of the model by perturbing the inputs to see how the predictions change\n",
        "  - The glassbox model is not exactly the same as the blackbox model, and it doesn't have to be\n",
        "\n",
        "+ The goal is to understand how the model has made a prediction for **a particular observation only**\n",
        "   + The glassbox has to behave similarly to the original blackbox **in a very narrow input space around a particular observation.**\n",
        "   + This idea is similar to a polynomial approximation of a function by using the Taylor series.\n",
        "     + The polynomial approximation is **not the same** as the original function, but it is **useful** if the input variable is close enough to the point of interest\n",
        "     + **Similarly, the glassbox is useful around the observation of interest**\n",
        "   + Sometimes, the glassbox part is called the \"proxy model\", making LIME a proxy method\n",
        "   \n",
        "- The default choice for the glassbox model is the [**lasso regression**](https://www.statisticshowto.com/lasso-regression/).\n",
        "  - Lasso regression is a form of linear regression\n",
        "  - Lasso regression penalizes less important features of your dataset and makes their respective coefficients zero, thereby eliminating them\n",
        "  - The linear model can copy the behavior of the original blackbox model\n",
        "    - Training input data: randomly distributed points around the point of interest\n",
        "    - Training output data: blackbox model's prediction on that set of random points\n",
        "\n",
        "+ **TLDR**: LIME is used to interpret blackbox models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e745aa-358a-4b5d-9e04-db9f1baab723",
      "metadata": {
        "id": "b6e745aa-358a-4b5d-9e04-db9f1baab723"
      },
      "source": [
        "### Toy Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22bfbd07-4fa8-4345-a991-629ef54495a2",
      "metadata": {
        "id": "22bfbd07-4fa8-4345-a991-629ef54495a2"
      },
      "source": [
        "Similar to the previous lab assignments, let's start with a toy dataset we have generated ourselves.\n",
        "\n",
        "\\begin{align*}\n",
        "(x_1, x_2, x_3) & \\sim \\mathrm{MultiNormal}\\Big(\n",
        "\\mu = [1, 0.5, 0], \\,\n",
        "\\mathrm{cov} =\n",
        "\\begin{bmatrix}\n",
        "6 & 3 & 0 \\\\\n",
        "3 & 6 & 0 \\\\\n",
        "0 & 0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "\\Big) \\\\\n",
        "\\\\\n",
        "y & \\sim \\mathrm{Normal}(\\mu = 10 + 0.8x_1 + 0.2x_{1}^{2} - 1.5\\max (x_{2},\\, 0) + 0.000001 x_{3} ,\\, \\sigma = 1.0) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "Since we know the true underlying distribution, let's use the regression function as our model to predict our dependent variable $y$ from the independent variables $(x_1,\\, x_2,\\, x_3)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0221274-4f53-4a9e-87df-7779841e2ad7",
      "metadata": {
        "id": "a0221274-4f53-4a9e-87df-7779841e2ad7"
      },
      "outputs": [],
      "source": [
        "def toy_model(x1, x2, x3):\n",
        "    b0, b1, b11, b2, b3 = 10.0, 0.8, 0.2, -1.5, 0.000001\n",
        "    y_mean = b0 + b1*x1 + b11*(x1**2) + b2*np.maximum(x2, 0) + b3*x3\n",
        "    return y_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22dbf1d-b224-4008-826e-d5cb7a4a7b75",
      "metadata": {
        "id": "b22dbf1d-b224-4008-826e-d5cb7a4a7b75"
      },
      "outputs": [],
      "source": [
        "def generate_data(n_obs = 300, seed = 42):\n",
        "    \"\"\"Draws random samples from the 2D multi-normal distribution\"\"\"\n",
        "    rng = np.random.default_rng(seed = seed)\n",
        "\n",
        "    # generate X\n",
        "    mean_X, cov_X = [1, 0.5, 0], [[6, 3, 0], [3, 6, 0], [0, 0, 1]]\n",
        "    X = rng.multivariate_normal(mean=mean_X, cov=cov_X, size=n_obs)\n",
        "\n",
        "    # generate y\n",
        "    mean_y = toy_model(*np.split(X, 3, axis=1))\n",
        "    std_y  = 1.0\n",
        "    y = rng.normal(loc = mean_y, scale = std_y)\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1a59edd-d766-444b-b5c6-cb465c6d4634",
      "metadata": {
        "id": "a1a59edd-d766-444b-b5c6-cb465c6d4634"
      },
      "source": [
        "- Please remember that:\n",
        "  - We can't always take this shortcut in real life because we almost never know **the true underlying distribution**, so we will use this toy example only to illustrate how LIME works.\n",
        "  - **Regression function** is defined as the conditional expected value of $y$ given $X = (x_1,\\, x_2,\\, x_3)$, which is $\\mathbb{E}(Y = y|X=(x_1,\\, x_2,\\, x_3))$ in this case.\n",
        "  - Our regression function here is:\n",
        "$$r(x_1,\\, x_2,\\, x_3) = 10 + 0.8x_1 + 0.2x_{1}^{2} - 1.5\\max (x_{2},\\, 0) + 0.000001 x_{3}$$\n",
        "\n",
        "This model is a very transparent model, and by \"transparent\" we mean we can easily see the model's prediction-making process. We can also easily see how each of the independent variables influence the model's prediction.  \n",
        "\n",
        "- Changes in $x_1$ will change the predicted value $\\hat{y}$.\n",
        "- Changes in $x_2$ will change the model prediction only when $x_2 \\geq 0$.\n",
        "- Changes in $x_3$ will not significantly change the model prediction.  \n",
        "\n",
        "Let's confirm our intuition with the visualization below, which does not include $x_3$, because human eyes cannot see more than three dimensions at once and we know that $x_3$ is not very important anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5a6498-5588-4171-a6ef-b9debb003857",
      "metadata": {
        "id": "3b5a6498-5588-4171-a6ef-b9debb003857"
      },
      "outputs": [],
      "source": [
        "X, y_true = generate_data()\n",
        "\n",
        "# visualize sample data\n",
        "fig_toy_model = limelab.visualize_data(X, y_true)\n",
        "# visualize model\n",
        "go_toy_model  = limelab.get_model_go(toy_model)\n",
        "\n",
        "fig_toy_model.add_trace(go_toy_model)\n",
        "fig_toy_model.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43d07f98-a24a-42c5-a9c3-e332dddc50c4",
      "metadata": {
        "id": "43d07f98-a24a-42c5-a9c3-e332dddc50c4"
      },
      "source": [
        "### Interpreting the Toy Model with LIME"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3122c557-e8c2-4070-8908-f04ece78a035",
      "metadata": {
        "id": "3122c557-e8c2-4070-8908-f04ece78a035"
      },
      "source": [
        "Here's how LIME uses lasso regression as a \"glassbox\" model to copy the local behavior around the point of interest. Please note that this is **a simplified version of the LIME method** that we are using only as a stepping stone to eventually reach the full version at the end of this lab.\n",
        "\n",
        "1. Generate many random samples from the domain that are near to the point of interest\n",
        "   - The term used here is \"perturbation\", where the perturbed $x$ ($x_{p}$) is defined as $x_{p} \\simeq x + \\epsilon$, where $\\epsilon$ is some noise.\n",
        "2. Calculate the weights of each random sample based on the distance to the point of interest\n",
        "   + The closer the distance, the larger the weight\n",
        "   + This makes it so random samples too far from the point of interest are ignored\n",
        "   + The default choice of the weight is the exponential kernel function $\\pi(x_{p}, x) = \\exp{\\left(-D(x_{p}, x)^{2} /\\sigma^{2}\\right)}$\n",
        "   + The choice of the distance function $-D(x_{p}, x)$ depends on the type of the data and the context of the model\n",
        "     - i.e. cosine distance for text, L2 distance for images, etc\n",
        "   + The size of the distance kernel $\\sigma$ for the samples should be small enough to capture the local model behavior, but it should be large enough to be resistant from the randomness in the process.\n",
        "3. Obtain the blackbox model's predictions for the generated random samples\n",
        "   - We use the blackbox predictions as the target variable ($y$) to train the glassbox model\n",
        "4. Train the glassbox model\n",
        "   + Use the \"perturbed\" random samples as the independent variables and the blackbox predictions as the target variable ($y$)\n",
        "   + As we mentioned earlier, the default choice for the glassbox model is the lasso regression\n",
        "   + The size of the L1 penalty for the lasso regression has to be chosen carefully\n",
        "5. Interpret the glassbox model instead of the blackbox model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2991f382-a986-43d3-aac9-f3bf996ae340",
      "metadata": {
        "id": "2991f382-a986-43d3-aac9-f3bf996ae340"
      },
      "outputs": [],
      "source": [
        "def make_perturbations(obs_point, n_samples=100, sample_range=2.0, seed=42):\n",
        "    \"\"\"generates perturbed samples around obs_point\"\"\"\n",
        "    rng = np.random.default_rng(seed = seed)\n",
        "    perturbed_samples = rng.uniform(low=obs_point - sample_range,\n",
        "                                    high=obs_point + sample_range,\n",
        "                                    size=(n_samples, len(obs_point)))\n",
        "    return perturbed_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9bedb3a-b80f-4ce3-b53b-35b6e879e2a6",
      "metadata": {
        "id": "d9bedb3a-b80f-4ce3-b53b-35b6e879e2a6"
      },
      "source": [
        "#### [Check-off 1] Weight Kernel\n",
        "\n",
        "As a warm-up, implement the `weight_kernel` function below.\n",
        "- `squared_distances`: squared Euclidean distances between the original observation point and the perturbed points\n",
        "- `weights`: the weight value from the exponential function $\\pi(x_{p}, x) = \\exp{\\left(-D(x_{p}, x)^{2} /\\sigma^{2}\\right)}$, where $\\sigma$ is the `kernel_size`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6d187c5-4b43-4f4e-a69d-2307f0610ed0",
      "metadata": {
        "id": "c6d187c5-4b43-4f4e-a69d-2307f0610ed0"
      },
      "outputs": [],
      "source": [
        "def weight_kernel(obs_point, perturbed_samples, kernel_size=2.50):\n",
        "    \"\"\"assigns appropriate weights to each perturbed sample point\n",
        "    according to its distance from the original point\"\"\"\n",
        "    # TODO:\n",
        "    # squared_distances = For all points, calculate the square of the Euclidean\n",
        "    # distance between the original observation point and the perturbed point.\n",
        "    # weights = calculate the exponential weights\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388a870d-2d10-4304-aabc-87ace47d997c",
      "metadata": {
        "id": "388a870d-2d10-4304-aabc-87ace47d997c"
      },
      "outputs": [],
      "source": [
        "obs_point_checkoff_1a = np.array([1.0, 2.0, -3.0])\n",
        "perturbed_samples_checkoff_1a = np.array(\n",
        "    [[ 1.72940745,  0.21528408, -4.11856051],\n",
        "     [-0.26251276,  0.7036236 , -1.75162197],\n",
        "     [ 2.69337999,  1.10629759, -1.72098175],\n",
        "     [ 2.55957077,  2.05188182, -4.0201416 ]])\n",
        "\n",
        "obs_point_checkoff_1b = np.array([-2.0, 3.0])\n",
        "perturbed_samples_checkoff_1b = np.array(\n",
        "    [[-1.27059255,  1.21528408],\n",
        "     [-3.11856051,  1.73748724],\n",
        "     [-3.2963764 ,  4.24837803],\n",
        "     [-0.30662001,  2.10629759]])\n",
        "\n",
        "weights_checkoff_1a = weight_kernel(obs_point_checkoff_1a, perturbed_samples_checkoff_1a)\n",
        "weights_checkoff_1b = weight_kernel(obs_point_checkoff_1b, perturbed_samples_checkoff_1b)\n",
        "print(f\"weights_checkoff_1a: {weights_checkoff_1a}\")\n",
        "print(f\"weights_checkoff_1b: {weights_checkoff_1b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d7552a-9767-4ebd-8a16-1d2d095f3dfd",
      "metadata": {
        "id": "c4d7552a-9767-4ebd-8a16-1d2d095f3dfd"
      },
      "source": [
        "The result should be the following\n",
        "```\n",
        "weights_checkoff_1a: [0.45160481 0.4614985  0.42812486 0.57343961]\n",
        "weights_checkoff_1b: [0.5516953  0.63430931 0.59556406 0.55621611]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b21888-5d43-44a6-bd4e-3bd4c636abe7",
      "metadata": {
        "id": "48b21888-5d43-44a6-bd4e-3bd4c636abe7"
      },
      "source": [
        "Let's pick the following two points from our toy model and see if LIME can copy the behavior of the original model around those points.\n",
        "\n",
        "- Point 1\n",
        "  - $(x_{1}^{(1)},\\, x_{2}^{(1)},\\, x_{3}^{(1)}) = (4.0,\\, 3.0,\\, 5.0)$\n",
        "  - Expected behavior: the model's prediction should change with the changes in $x_1$ and $x_2$ but not with $x_3$\n",
        "\n",
        "+ Point 2\n",
        "  + $(x_{1}^{(2)},\\, x_{2}^{(2)},\\, x_{3}^{(2)}) = (-2.0,\\, -4.0,\\, -3.0)$\n",
        "  + Expected behavior: the model's prediction should changes with the changes in $x_1$ but not with $x_2$ (because its coefficient is negative) and $x_3$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20654eb0-d881-4395-9ee0-b830cef85054",
      "metadata": {
        "id": "20654eb0-d881-4395-9ee0-b830cef85054"
      },
      "source": [
        "#### [Check-off 2] LIME for Tabular Data\n",
        "\n",
        "Implement the `get_glassbox_coef` function below.\n",
        "- `obs_point`: can be a point in a 2D space like \\[1.0, 2.0\\] or a point in a 3D space like \\[1.0, 2.0, 3.0\\].\n",
        "- `perturbed_samples`: perturbed samples obtained from the function `make_perturbations`. Make sure you input the correct default values for the keyword arguments `n_samples` and `sample_range`\n",
        "- `perturbed_predictions`: predictions on the perturbed samples from the toy model. Remember that the toy model can expect either a 2D point like \\[1.0, 2.0\\] or a 3D like \\[1.0, 2.0, 3.0\\]. Hint: the [np.split function](https://numpy.org/doc/stable/reference/generated/numpy.split.html) can be very useful.\n",
        "- `perturbed_weights`: the weights on each perturbed samples from the function `weight_kernel`. Make sure you input the correct default values for the keyword argument `kernel_size`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e8cbaa-7404-4c61-8dda-1d0561893761",
      "metadata": {
        "id": "b6e8cbaa-7404-4c61-8dda-1d0561893761"
      },
      "outputs": [],
      "source": [
        "def get_glassbox_coef(toy_model, obs_point,\n",
        "                      n_samples=100, sample_range=2.0, kernel_size=2.50):\n",
        "    \"\"\"returns the glassbox model's coefficients\"\"\"\n",
        "    # TODO:\n",
        "    # perturbed_samples     = do something with make_perturbations()\n",
        "    # perturbed_predictions = do something with toy_model(*np.split())\n",
        "    # perturbed_weights     = do something with weight_kernel()\n",
        "\n",
        "    # glassbox code\n",
        "    # alpha is the size of L1 penalty\n",
        "    glassbox = Lasso(alpha=0.05).fit(perturbed_samples,\n",
        "                                     perturbed_predictions,\n",
        "                                     sample_weight=perturbed_weights)\n",
        "\n",
        "    return (glassbox.coef_.reshape(-1), glassbox.intercept_[0],\n",
        "            perturbed_samples, perturbed_predictions, perturbed_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563c4134-530d-4ad0-8b67-c5d668be085f",
      "metadata": {
        "id": "563c4134-530d-4ad0-8b67-c5d668be085f"
      },
      "outputs": [],
      "source": [
        "obs1 = np.array([4.0, 3.0, 5.0])\n",
        "obs2 = np.array([-1.0, -5.0, 2.0])\n",
        "\n",
        "coef_glassbox1, inter_glassbox1, perturbed_samples1, perturbed_predictions1, perturbed_weights1 \\\n",
        "    = get_glassbox_coef(toy_model, obs1)\n",
        "coef_glassbox2, inter_glassbox2, perturbed_samples2, perturbed_predictions2, perturbed_weights2 \\\n",
        "    = get_glassbox_coef(toy_model, obs2)\n",
        "\n",
        "print(f\"coef_glassbox1: {coef_glassbox1}\")\n",
        "print(f\"coef_glassbox2: {coef_glassbox2}\")\n",
        "\n",
        "np.testing.assert_almost_equal(coef_glassbox1, [2.32930351613909, -1.436022319324604, 0.])\n",
        "np.testing.assert_almost_equal(coef_glassbox2, [0.33229950437092026, 0., 0.])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fcc3e8-ff05-4031-997c-7e5b1a07f582",
      "metadata": {
        "id": "33fcc3e8-ff05-4031-997c-7e5b1a07f582"
      },
      "source": [
        "The result should be the following\n",
        "```\n",
        "coef_glassbox1: [ 2.32930352 -1.43602232 -0.        ]\n",
        "coef_glassbox2: [ 0.3322995  0.        -0.       ]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8432aca-2a47-4976-a272-0b99df20de89",
      "metadata": {
        "id": "e8432aca-2a47-4976-a272-0b99df20de89"
      },
      "source": [
        "You should be able to see that only the first and second coefficients are non-zero for `obs1` and that only the first coefficient is non-zero for `obs2`. We can also visually confirm that intuition with the 3D interactive plot below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016385f3-e9c8-47fc-9a96-1f7512a04420",
      "metadata": {
        "id": "016385f3-e9c8-47fc-9a96-1f7512a04420"
      },
      "outputs": [],
      "source": [
        "X, y_true = generate_data()\n",
        "# visualize sample data\n",
        "fig_toy_lime = limelab.visualize_data(X, y_true)\n",
        "# visualize the toy model\n",
        "go_toy_model = limelab.get_model_go(toy_model)\n",
        "\n",
        "# visualize glass model\n",
        "go_glass1    = limelab.get_glass_go(obs1, 2.0, coef_glassbox1, inter_glassbox1)\n",
        "go_glass2    = limelab.get_glass_go(obs2, 2.0, coef_glassbox2, inter_glassbox2)\n",
        "\n",
        "# visualize perturbed samples\n",
        "go_perturb1  = limelab.get_perturb_go(perturbed_samples1,\n",
        "                                      perturbed_predictions1,\n",
        "                                      perturbed_weights1)\n",
        "go_perturb2  = limelab.get_perturb_go(perturbed_samples2,\n",
        "                                      perturbed_predictions2,\n",
        "                                      perturbed_weights2)\n",
        "\n",
        "fig_toy_lime.add_traces([go_toy_model, go_glass1, go_glass2, go_perturb1, go_perturb2])\n",
        "fig_toy_lime.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d33936f7-062e-4dec-8f94-66c29e76a369",
      "metadata": {
        "id": "d33936f7-062e-4dec-8f94-66c29e76a369"
      },
      "source": [
        "### Global vs. Local Behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174b48c8-4944-44d3-baa9-342931665884",
      "metadata": {
        "id": "174b48c8-4944-44d3-baa9-342931665884"
      },
      "source": [
        "A model's local behavior is not always the same as its global behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0cc4b1-d057-41a8-a824-052060340256",
      "metadata": {
        "id": "6c0cc4b1-d057-41a8-a824-052060340256"
      },
      "outputs": [],
      "source": [
        "def difficult_model(x1, x2):\n",
        "    \"\"\"a blackbox model of which global behavior is\n",
        "    very different from its local behavior\"\"\"\n",
        "    # important global behavior + unimportant local noise\n",
        "    y_mean = 8.0/(1.0 + np.exp(-2.0*x1)) + 0.5*np.sin(x1)*np.sin(x2)\n",
        "    return y_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e6c3c55-7aa7-4978-8377-ecdd49f4808f",
      "metadata": {
        "id": "4e6c3c55-7aa7-4978-8377-ecdd49f4808f"
      },
      "outputs": [],
      "source": [
        "obs3 = np.array([-5.0, 3.0])\n",
        "obs4 = np.array([3.0, 1.5])\n",
        "\n",
        "coef_glassbox3, inter_glassbox3, perturbed_samples3, perturbed_predictions3, perturbed_weights3 \\\n",
        "    = get_glassbox_coef(difficult_model, obs3, sample_range=1.0)\n",
        "coef_glassbox4, inter_glassbox4, perturbed_samples4, perturbed_predictions4, perturbed_weights4 \\\n",
        "    = get_glassbox_coef(difficult_model, obs4, sample_range=1.0)\n",
        "\n",
        "print(f\"coef_glassbox3: {coef_glassbox3}\")\n",
        "print(f\"coef_glassbox4: {coef_glassbox4}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e8fd55-91fd-42f0-9d85-ec06489e24f7",
      "metadata": {
        "id": "19e8fd55-91fd-42f0-9d85-ec06489e24f7"
      },
      "source": [
        "The result should be the following. If not, go back to **\\[check-off 2\\]** and fix the `get_glassbox_coef` function.\n",
        "```\n",
        "coef_glassbox3: [ 0.         -0.20230021]\n",
        "coef_glassbox4: [-0.15756295 -0.        ]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5317a481-7326-4436-aaa3-99f68b331f12",
      "metadata": {
        "id": "5317a481-7326-4436-aaa3-99f68b331f12"
      },
      "source": [
        "From the LIME interpretation only, you might be mislead to think that $x_1$ is not an important factor for `obs3` and that the larger values for $x_1$ would make the predicted value smaller for `obs4`. However, **that is absolutely not the case if you look at the global behavior**. In cases like these, you have to be careful because LIME only shows you the local behavior of the blackbox model, which might be vastly different from the global behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89bf58f2-3ab7-45c2-998d-74062fbdc0e6",
      "metadata": {
        "id": "89bf58f2-3ab7-45c2-998d-74062fbdc0e6"
      },
      "outputs": [],
      "source": [
        "fig_difficult = limelab.visualize_difficult_model(difficult_model)\n",
        "go_glass3     = limelab.get_glass_go(obs3, 2.0, coef_glassbox3, inter_glassbox3)\n",
        "go_glass4     = limelab.get_glass_go(obs4, 2.0, coef_glassbox4, inter_glassbox4)\n",
        "go_perturb3   = limelab.get_perturb_go(perturbed_samples3, perturbed_predictions3, perturbed_weights3)\n",
        "go_perturb4   = limelab.get_perturb_go(perturbed_samples4, perturbed_predictions4, perturbed_weights4)\n",
        "fig_difficult.add_traces([go_glass3, go_glass4, go_perturb3, go_perturb4])\n",
        "fig_difficult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb97e864-c247-4dfc-8576-a90208340be9",
      "metadata": {
        "id": "fb97e864-c247-4dfc-8576-a90208340be9"
      },
      "source": [
        "## LIME on image recognition models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a72cc8-e9e2-4c48-b3af-7427c7126c9a",
      "metadata": {
        "id": "c0a72cc8-e9e2-4c48-b3af-7427c7126c9a"
      },
      "source": [
        "The full version of LIME has only two more steps than the simplified version we have used above.\n",
        "- Map the observation point around which you want to interpret the model to an interpretable space\n",
        "- Generate perturbed samples from the interpretable space\n",
        "\n",
        "In our toy examples, our input space had only two or three dimensions, simple enough to interpret directly without any need to introduce yet another space just for the purpose of interpretation. However, when the input data has a lot more dimensions, it would be easier to make the interpretation in a lower dimensional space.\n",
        "\n",
        "Here are some examples of when your data might be high-dimensional and you want to make it lower-dimensional.\n",
        "\n",
        "- High dimensional tabular data\n",
        "  - It is easier to work with fewer features that are more important than the others\n",
        "  - We could even use traditional dimension reduction methods such as PCA\n",
        "- Image\n",
        "  - Treat groups of nearby pixels as a single unit to convey graphical information\n",
        "  - Analyzing the role of each individual pixel value does not make sense in the human visual perception\n",
        "  \n",
        "Another reason for using a separate interpretable space is that evaluating how the model prediction depends on each features' values often fails to provide insights into how the model understands the data. Some other interpretation methods include [saliency maps](https://arxiv.org/abs/1312.6034), which [don't always work](https://www.medrxiv.org/content/10.1101/2020.07.28.20163899v2.full), and [gradient-based methods](https://arxiv.org/abs/1810.03292).  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a91a389-efc8-4a1e-8e00-8260e55454ae",
      "metadata": {
        "id": "6a91a389-efc8-4a1e-8e00-8260e55454ae"
      },
      "source": [
        "### Image perturbation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74f026e7-52e8-402b-9468-3504184c1d94",
      "metadata": {
        "id": "74f026e7-52e8-402b-9468-3504184c1d94"
      },
      "source": [
        "Just as a review, image data can be manipulated with NumPy arrays where the dimensions represent the height, width, and color channel. (If you are curious, the astronaut in the example image is [Eileen Collins](https://scikit-image.org/docs/stable/api/skimage.data.html#skimage.data.astronaut).)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8382a315-2c29-48d2-b882-c7db9d7e058a",
      "metadata": {
        "id": "8382a315-2c29-48d2-b882-c7db9d7e058a"
      },
      "outputs": [],
      "source": [
        "astronaut_img = astronaut()[::2,::2]/255.0\n",
        "print(f\"array shape: {astronaut_img.shape}\")\n",
        "print(f\"array dtype: {astronaut_img.dtype}\")\n",
        "\n",
        "fig_astronaut, ax_astronaut = plt.subplots(figsize = (4, 4))\n",
        "ax_astronaut.imshow(astronaut_img)\n",
        "ax_astronaut.set_axis_off()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e40180d-1ffc-42fe-b58e-c2e0c43c49d1",
      "metadata": {
        "id": "2e40180d-1ffc-42fe-b58e-c2e0c43c49d1"
      },
      "source": [
        "Now, we will divide up the image into multiple **superpixels**, which are groups of nearby pixels that convey some kind of visual information. For examples, superpixels may represent entire objects in images.\n",
        "\n",
        "Naturally, there are [many image segmentation algorithms available](https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_segmentations.html#sphx-glr-auto-examples-segmentation-plot-segmentations-py), but we have to pick one to begin with and `quickshift` is the default algorithm chosen by the authors of the original LIME paper and the Python package, so we will use that one too.\n",
        "\n",
        "Just like many other community-developed open-source scientific packages, the scikit-image package also has some bugs. For example, the `quickshift` function returns an error when the input is in the 32 bit floating point format in the linux environment like Google Colab. Thus, we will use the `safe_quickshift` function defined below as a temporary workaround."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c63d1c53-ca7f-4c53-b983-cd10311ab293",
      "metadata": {
        "id": "c63d1c53-ca7f-4c53-b983-cd10311ab293"
      },
      "outputs": [],
      "source": [
        "def safe_quickshift(original_image, **kwargs):\n",
        "    return quickshift(np.float64(original_image), **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e8c8c1b-a226-4b8e-87f8-49a68be4520d",
      "metadata": {
        "id": "9e8c8c1b-a226-4b8e-87f8-49a68be4520d"
      },
      "source": [
        "By convention, the segmentation is represented by a 2D NumPy array of the same pixel size of the original array, in which each segment area is denoted by its own ID Number.\n",
        "\n",
        "The `quickshift` function has the following three arguments. You can look up the [official documentation](https://scikit-image.org/docs/stable/api/skimage.segmentation.html#skimage.segmentation.quickshift) for more instructions.\n",
        "\n",
        "- `kernel_size`\n",
        "  - Width of [Gaussian kernel](https://medium.com/@akumar5/computer-vision-gaussian-filter-from-scratch-b485837b6e09) used in smoothing the sample density  \n",
        "  - Larger size means fewer segments\n",
        "+ `max_dist`\n",
        "  + Cut-off point for data distances\n",
        "  + Larger distance means fewer segments\n",
        "- `ratio`\n",
        "  - This ratio considers pixel grouping based on color versus spatial organization\n",
        "  - We recommend a ratio of 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f901c38-b0e9-4fa1-8539-8580fd5c447d",
      "metadata": {
        "id": "7f901c38-b0e9-4fa1-8539-8580fd5c447d"
      },
      "outputs": [],
      "source": [
        "astronaut_seg = safe_quickshift(astronaut_img, kernel_size=8, max_dist=250, ratio=0.5)\n",
        "\n",
        "fig_astronaut_seg, ax_astronaut_seg = plt.subplots(figsize = (4, 4))\n",
        "ax_astronaut_seg.imshow(mark_boundaries(astronaut_img, astronaut_seg))\n",
        "ax_astronaut_seg.set_axis_off()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06cd6c8-3732-4808-88f1-82d6a9992aaf",
      "metadata": {
        "id": "f06cd6c8-3732-4808-88f1-82d6a9992aaf"
      },
      "outputs": [],
      "source": [
        "print(f\"segmentation array shape: {astronaut_seg.shape}\")\n",
        "print(f\"segmentation array dtype: {astronaut_seg.dtype}\")\n",
        "print(f\"segmentation array: \\n\", np.array2string(astronaut_seg, edgeitems=10), \"\\n\")\n",
        "\n",
        "n_segments = len(np.unique(astronaut_seg))\n",
        "print(f\"Number of segments: {n_segments}\")\n",
        "print(f\"Segment IDs:\\n {np.unique(astronaut_seg)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3965348b-52f8-40f0-b0f7-278064d9b789",
      "metadata": {
        "id": "3965348b-52f8-40f0-b0f7-278064d9b789"
      },
      "source": [
        "Since the segment ID is just an integer, we can easily create a Boolean mask to choose a specific segment area and do some operations on the original image array. For example, the ID number for the segment area around the space shuttle (not the fuel tank, but the shuttle itself) is 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531fdb25-a187-4431-a564-9138400a8367",
      "metadata": {
        "id": "531fdb25-a187-4431-a564-9138400a8367"
      },
      "outputs": [],
      "source": [
        "# creating a Boolean mask with element-wise NumPy operator\n",
        "mask_for_specific_segment_area = (astronaut_seg == 5)\n",
        "print(f\"mask shape: {mask_for_specific_segment_area.shape}\")\n",
        "print(f\"mask dtype: {mask_for_specific_segment_area.dtype}\")\n",
        "\n",
        "# Blank image filled with magenta\n",
        "# Can you see how [1.0, 0.0, 1.0] is broadcast into the whole array?\n",
        "shuttle_img = np.zeros(astronaut_img.shape, dtype=np.float64) + np.array([1.0, 0.0, 1.0])\n",
        "\n",
        "# Copy the astonaut image only in the area selected by the Boolean mask\n",
        "shuttle_img[mask_for_specific_segment_area] = astronaut_img[mask_for_specific_segment_area]\n",
        "\n",
        "fig_seg_mask, ax_seg_mask = plt.subplots(1, 2, figsize=(8, 4))\n",
        "ax_seg_mask[0].imshow(mask_for_specific_segment_area, cmap = \"Blues\")\n",
        "ax_seg_mask[0].grid()\n",
        "ax_seg_mask[1].imshow(shuttle_img)\n",
        "ax_seg_mask[1].set_axis_off()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d992a24-44ab-4a8a-9572-f5298dedc3d2",
      "metadata": {
        "id": "3d992a24-44ab-4a8a-9572-f5298dedc3d2"
      },
      "source": [
        "What we can do now is **fudge** each segment area, which is to **overwrite each pixel value with the mean values for each RGB color channel of all pixels in the segment**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9598b338-5ffb-45cf-a6e9-f9f842ee2a3d",
      "metadata": {
        "id": "9598b338-5ffb-45cf-a6e9-f9f842ee2a3d"
      },
      "outputs": [],
      "source": [
        "def fudge_image(original_image, segmentation):\n",
        "    \"\"\"fudge each segments of the image\"\"\"\n",
        "    fudged_image = original_image.copy()\n",
        "    for each_seg in np.unique(segmentation):\n",
        "        each_seg_area_mask = (segmentation == each_seg)\n",
        "        fudged_image[each_seg_area_mask] = np.mean(\n",
        "            fudged_image[each_seg_area_mask], axis=0)\n",
        "\n",
        "    return fudged_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e18e4204-d62c-48b1-a9f7-ab85bb8a77f2",
      "metadata": {
        "id": "e18e4204-d62c-48b1-a9f7-ab85bb8a77f2"
      },
      "outputs": [],
      "source": [
        "def display_original_and_fudged_image(original_image, fudged_image,\n",
        "                                      segmentation, mode=None):\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    ax[0].imshow(original_image)\n",
        "    ax[0].set_title(\"Orignal Image\")\n",
        "    ax[1].imshow(mark_boundaries(original_image, segmentation, mode=mode))\n",
        "    ax[1].set_title(\"Segmented\")\n",
        "    ax[2].imshow(mark_boundaries(fudged_image, segmentation, mode=mode))\n",
        "    ax[2].set_title(\"Segmented and fudged\")\n",
        "    ax[0].set_axis_off()\n",
        "    ax[1].set_axis_off()\n",
        "    ax[2].set_axis_off()\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90dce0c8-782b-4924-96f5-a9f98b8765b4",
      "metadata": {
        "id": "90dce0c8-782b-4924-96f5-a9f98b8765b4"
      },
      "outputs": [],
      "source": [
        "astronaut_fudged = fudge_image(astronaut_img, astronaut_seg)\n",
        "fig_astronaut_fudgedd, ax_astronaut_fudgedd = \\\n",
        "    display_original_and_fudged_image(astronaut_img, astronaut_fudged, astronaut_seg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4111f1-76fb-465f-a08e-a762cf3dbe46",
      "metadata": {
        "id": "2b4111f1-76fb-465f-a08e-a762cf3dbe46"
      },
      "source": [
        "Now it's time to perturb the image. Here, we also need a **segmentation vector**, which is a vector that tells us how exactly the orginal image has been perturbed. If the $i$-th element of the vector is 0, it means the $i$-th segment in the image is fudged, while 1 means that the segment has not been fudged.\n",
        "\n",
        "For example, if the segmentation vector is $\\vec{s} = [1, 1, 0, 0, 1]$, then only the third and fourth segments are fudged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6112896f-187b-4789-8d0b-6cf3e193c7d9",
      "metadata": {
        "id": "6112896f-187b-4789-8d0b-6cf3e193c7d9"
      },
      "outputs": [],
      "source": [
        "def perturb_image(original_image, fudged_image, segmentation,\n",
        "                  seed=None):\n",
        "    \"\"\"return the perturbed image and the segmentation vector\"\"\"\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "\n",
        "    n_segments = len(np.unique(segmentation))\n",
        "    # randomly choose image segments to perturb\n",
        "    segmentation_vector_mask = rng.choice([True, False], size = n_segments)\n",
        "\n",
        "    perturbed_image = fudged_image.copy()\n",
        "    sampled_area_mask = segmentation_vector_mask[segmentation]\n",
        "    perturbed_image[sampled_area_mask] = original_image[sampled_area_mask]\n",
        "\n",
        "    return perturbed_image, segmentation_vector_mask.astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280e84c9-5e74-4dc3-8751-18139fb03b67",
      "metadata": {
        "id": "280e84c9-5e74-4dc3-8751-18139fb03b67"
      },
      "outputs": [],
      "source": [
        "def display_perturbations(original_image, fudged_image, segmentation,\n",
        "                          initial_seed=None, show_vectors=True):\n",
        "    \"\"\"display twelve randomly perturbed images\"\"\"\n",
        "    fig, ax = plt.subplots(3, 4, figsize=(12, 9))\n",
        "    rng = np.random.default_rng(seed=initial_seed)\n",
        "\n",
        "    for i, j in itertools.product(range(3), range(4)):\n",
        "        seed = rng.integers(2**63)\n",
        "        perturbed_image, seg_vec = perturb_image(original_image, fudged_image, segmentation, seed)\n",
        "\n",
        "        ax[i, j].imshow(perturbed_image)\n",
        "        ax[i, j].set_axis_off()\n",
        "\n",
        "        if show_vectors:\n",
        "            ax[i, j].text(int(0.05*perturbed_image.shape[0]), int(0.9*perturbed_image.shape[0]),\n",
        "                          str(seg_vec), fontsize=8, bbox = dict(color=\"White\", alpha=0.75))\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d3715b6-5db8-427b-97af-d0478808ce5d",
      "metadata": {
        "id": "8d3715b6-5db8-427b-97af-d0478808ce5d"
      },
      "outputs": [],
      "source": [
        "fig_astronaut_perturb, ax_astronaut_perturb = \\\n",
        "    display_perturbations(astronaut_img, astronaut_fudged, astronaut_seg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd9b345-8624-4d9d-a92c-d8d53b46c408",
      "metadata": {
        "id": "6cd9b345-8624-4d9d-a92c-d8d53b46c408"
      },
      "source": [
        "PS. When you use the `make_boundries()` function, you might get a warning message like this when `mode=\"subpixel\"`.\n",
        "\n",
        "```\n",
        "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
        "```\n",
        "\n",
        "That's because `mark_boundaries()` uses `scipy.ndimage.zoom` internally to enlarge the original image of the size $(n,\\, n)$ to obtain an image of the size $(2n-1, \\, 2n-1)$, and [that zoom function uses cubic interpolation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.zoom.html), which can sometimes result in the pixel values below 0.0 or above 1.0."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd6dc23-3d46-4867-80d4-374c6da268c2",
      "metadata": {
        "id": "1fd6dc23-3d46-4867-80d4-374c6da268c2"
      },
      "source": [
        "### Create a CNN Model using the CIFAR10 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eef1e16c-3c2c-4353-b84a-6abaf1bca6df",
      "metadata": {
        "id": "eef1e16c-3c2c-4353-b84a-6abaf1bca6df"
      },
      "source": [
        "First let's load the CIFAR10 dataset. Here, we only need to load the test set, because TAs have already trained the model for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a4ece4f-c6d4-4e2d-bcc3-0baf800e1deb",
      "metadata": {
        "id": "9a4ece4f-c6d4-4e2d-bcc3-0baf800e1deb"
      },
      "outputs": [],
      "source": [
        "# No need to load the train set\n",
        "image_test_full, label_test_full = unpickle_CIFAR(f\"{data_path}/test\")\n",
        "\n",
        "# Keep cats and dogs only and throw away the other classes\n",
        "cifar_class_list = [\"cat\", \"dog\"]\n",
        "image_test_uint, label_test = get_subset(image_test_full, label_test_full,\n",
        "                                         class_list=cifar_class_list,\n",
        "                                         num=None)\n",
        "\n",
        "# Shuffle and normalize\n",
        "seed = 42\n",
        "image_test_uint,  label_test  = shuffle_data(image_test_uint, label_test, seed)\n",
        "image_test = np.float32(image_test_uint/255.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MDXjea8PmbFq"
      },
      "id": "MDXjea8PmbFq"
    },
    {
      "cell_type": "markdown",
      "id": "91e42751-a4e9-418d-bdad-7ecc5e075f0e",
      "metadata": {
        "id": "91e42751-a4e9-418d-bdad-7ecc5e075f0e"
      },
      "source": [
        "**We have prepared a model for you. All you need to do is just import the model via the h5 file.** If you want to build the model by yourself, you can copy and paste the code in this markdown cell.\n",
        "\n",
        "<details> <summary> <b> Click here for the code </b> </summary>\n",
        "\n",
        "```python\n",
        "cnn_model = tf.keras.Sequential(\n",
        "    layers = [\n",
        "        # First Convolution Layer\n",
        "        tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=(2, 2), padding=\"same\", name=\"Conv1\"),\n",
        "        tf.keras.layers.BatchNormalization(axis=[1, 2], momentum=0, center=False, scale=False, name=\"Conv1-Norm\"),\n",
        "        tf.keras.layers.LeakyReLU(name=\"Conv1-LeakyReLU\"),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\", name=\"Conv1-Pool\"),\n",
        "        # Second Convolution Layer\n",
        "        tf.keras.layers.Conv2D(filters=20, kernel_size=5, strides=(2, 2), padding=\"same\", name=\"Conv2\"),\n",
        "        tf.keras.layers.BatchNormalization(axis=[1, 2], momentum=0, center=False, scale=False, name=\"Conv2-Norm\"),\n",
        "        tf.keras.layers.LeakyReLU(name=\"Conv2-LeakyReLU\"),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\", name=\"Conv2-Pool\"),\n",
        "        # Third Convolution Layer\n",
        "        tf.keras.layers.Conv2D(filters=20, kernel_size=3, strides=(1, 1), padding=\"same\", name=\"Conv3\"),\n",
        "        tf.keras.layers.BatchNormalization(axis=[1, 2], momentum=0, center=False, scale=False, name=\"Conv3-Norm\"),\n",
        "        tf.keras.layers.LeakyReLU(name=\"Conv3-LeakyReLU\"),\n",
        "        # Three Dense Layers\n",
        "        tf.keras.layers.Flatten(name=\"Flatten\"),\n",
        "        tf.keras.layers.Dense(160, activation=\"leaky_relu\", name=\"Dense1\"),\n",
        "        tf.keras.layers.Dropout(rate=0.3, name=\"Dropout1\"),\n",
        "        tf.keras.layers.Dense(20, activation=\"leaky_relu\", name=\"Dense2\"),\n",
        "        tf.keras.layers.Dropout(rate=0.3, name=\"Dropout2\"),\n",
        "        tf.keras.layers.Dense(2, activation=\"softmax\", name=\"Dense3\"),\n",
        "    ],\n",
        "    name = \"cnn_model\"\n",
        ")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9404d4c-e171-44d1-a86b-340997e337bd",
      "metadata": {
        "id": "d9404d4c-e171-44d1-a86b-340997e337bd"
      },
      "outputs": [],
      "source": [
        "with open(f\"{model_path}/cnn_model_export.json\", \"r\") as readfile:\n",
        "    cnn_model_export = json.load(readfile)\n",
        "\n",
        "cnn_model = tf.keras.models.model_from_json(cnn_model_export)\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3c76615-4739-446a-8eb1-b97fb55b788f",
      "metadata": {
        "id": "e3c76615-4739-446a-8eb1-b97fb55b788f"
      },
      "source": [
        "**You don't have to train the model at all, because you can just load the pretrained weights, prepared by the TAs,** although you could copy and paste the code in this cell if you really want to do it yourself.\n",
        "\n",
        "<details> <summary> <b> Click here for the code </b> </summary>\n",
        "\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "\n",
        "cnn_model.compile(optimizer=optimizer,\n",
        "                  loss=loss,\n",
        "                  metrics=metrics)\n",
        "\n",
        "cnn_model.fit(x = image_train, y = oh_label_train,\n",
        "              epochs = 30, batch_size = 64,\n",
        "              validation_data = (image_test, oh_label_test), validation_freq = 5)\n",
        "\n",
        "cnn_model_export = cnn_model.to_json()\n",
        "with open(f\"{model_path}/cnn_model_export.json\", \"w\", encoding = \"utf-8\") as outfile:\n",
        "    json.dump(cnn_model_export, outfile)\n",
        "    \n",
        "# you can change the name of the exported weights\n",
        "cnn_model.save_weights(f\"{model_path}/cnn_model_weights_TA\")\n",
        "```\n",
        "</details>    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d8c5e0-fdc4-4e37-884f-d71e9b77f543",
      "metadata": {
        "id": "97d8c5e0-fdc4-4e37-884f-d71e9b77f543"
      },
      "outputs": [],
      "source": [
        "cifar_class_array = np.array(cifar_class_list)\n",
        "predicted_labels = cifar_class_array[tf.argmax(cnn_model(image_test), axis=1)]\n",
        "\n",
        "model_accuracy = np.sum(label_test == predicted_labels)/len(label_test)\n",
        "print(f\"accuracy BEFORE loading the weights = {model_accuracy:0.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e53bc2-699e-4657-b996-c03571c66dc6",
      "metadata": {
        "id": "48e53bc2-699e-4657-b996-c03571c66dc6"
      },
      "source": [
        "The trained model should have a higher accuracy than the untrained one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4ee1fa-7a9e-4709-869d-bbfa1ff80444",
      "metadata": {
        "id": "5e4ee1fa-7a9e-4709-869d-bbfa1ff80444"
      },
      "outputs": [],
      "source": [
        "# load the pretrained weights\n",
        "cnn_model.load_weights(f\"{model_path}/cnn_model_weights_TA.weights.h5\")\n",
        "\n",
        "cifar_class_array = np.array(cifar_class_list)\n",
        "predicted_labels = cifar_class_array[tf.argmax(cnn_model(image_test), axis=1)]\n",
        "\n",
        "model_accuracy = np.sum(label_test == predicted_labels)/len(label_test)\n",
        "print(f\"Accuracy AFTER loading the weights = {model_accuracy:0.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2703c4f-407a-4c57-b324-a88b10b3854f",
      "metadata": {
        "id": "d2703c4f-407a-4c57-b324-a88b10b3854f"
      },
      "source": [
        "### Throw LIME at Cats and Dogs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd50c739-3323-4e24-a05f-665e8ab20523",
      "metadata": {
        "id": "bd50c739-3323-4e24-a05f-665e8ab20523"
      },
      "source": [
        "Please don't actually throw limes at your pets. I don't think they will like it. However, you can always apply the LIME algorithm to your image recognition model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "900e931f-1b60-42be-b3cb-a558e697c15d",
      "metadata": {
        "id": "900e931f-1b60-42be-b3cb-a558e697c15d"
      },
      "source": [
        "Here are the specific steps to apply the LIME algorithm on a blackbox model for image recognition.\n",
        "\n",
        "1. Generate many randomly perturbed segmented images.\n",
        "   - Segment the original image into superpixels with the `safe_quickshift()` function.\n",
        "   - Fudge the original image with the `fudge_image()` function.\n",
        "   - Generate many perturbed images by calling the `perturb_image()` function many times.\n",
        "   - Remember to collect the segmentation vectors.\n",
        "2. Calculate the weights of each random perturbation based on the distance from the original image.\n",
        "   + Use the `image_weight_kernel()` function.\n",
        "3. Obtain the blackbox model's predicted probabilities for a particular label of your choice.\n",
        "   - This is because you are curious about why the model has predicted that particular label.\n",
        "   - Let's use the true label of the image in this lab.\n",
        "   - You can also use the incorrect label of the image, if you are curious why the model has made the incorrect prediction.\n",
        "4. Train the glassbox model\n",
        "   + Use the segmentation vector as the feature variables and the blackbox probability as the target variable.\n",
        "   + The default choice for the glassbox model is the lasso regression again\n",
        "   + The size of the L1 penalty for the lasso regression has to be chosen carefully.\n",
        "5. Interpret the glassbox model instead of the blackbox model\n",
        "   - You can visualize the LIME interpretation by fudging and grayscale the unimportant segments of the image.\n",
        "   - In the \"glassbox_image\", the important segments will remain in the original color, while the unimportant segments will be fudged and grayscaled.\n",
        "   - Use the `rgb2gray_3ch()` function for grayscaling.\n",
        "   - Use the `get_glassbox_iamge()` function for the lasso regression and to generate the glassbox image."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's walk through all these points! First, generate many randomly perturbed segmented CIFAR images."
      ],
      "metadata": {
        "id": "ohZ1HPuEAND1"
      },
      "id": "ohZ1HPuEAND1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a668fec1-f474-4fb9-b1d2-706ee7944fc4",
      "metadata": {
        "id": "a668fec1-f474-4fb9-b1d2-706ee7944fc4"
      },
      "outputs": [],
      "source": [
        "cifar_class_list = [\"cat\", \"dog\"]\n",
        "cifar_class_dict = {\"cat\":0, \"dog\":1}\n",
        "cifar_seg_params = dict(kernel_size=3, max_dist=7, ratio=0.5)\n",
        "\n",
        "cifar_index = 219\n",
        "cifar_image = image_test[cifar_index]\n",
        "cifar_label = label_test[cifar_index]\n",
        "cifar_label_index = cifar_class_dict[cifar_label]\n",
        "\n",
        "print(f\"cifar_label: {cifar_label}\")\n",
        "print(f\"cifar_label_index: {cifar_label_index}\")\n",
        "\n",
        "cifar_seg = safe_quickshift(cifar_image, **cifar_seg_params)\n",
        "cifar_fudged = fudge_image(cifar_image, cifar_seg)\n",
        "\n",
        "print(f\"Number of segments: {len(np.unique(cifar_seg))}\")\n",
        "\n",
        "fig_organge_cat, ax_orange_cat = \\\n",
        "    display_original_and_fudged_image(cifar_image, cifar_fudged, cifar_seg,\n",
        "                                      mode=\"subpixel\")\n",
        "fig_test_perturb, ax_test_perturb = display_perturbations(cifar_image, cifar_fudged, cifar_seg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, calculate the weights of each random perturbation based on the distance from the original image."
      ],
      "metadata": {
        "id": "vb6wUQ-Q_Trm"
      },
      "id": "vb6wUQ-Q_Trm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e987550-8b88-4988-be49-6240efbe471b",
      "metadata": {
        "id": "9e987550-8b88-4988-be49-6240efbe471b"
      },
      "outputs": [],
      "source": [
        "def image_weight_kernel(original_image, perturbed_image, kernel_size=5.0):\n",
        "    # for a single RGB colored image only\n",
        "    assert original_image.ndim == 3\n",
        "    assert original_image.shape[2] == 3\n",
        "\n",
        "    image_squared_distance = np.sum((original_image - perturbed_image)**2)\n",
        "    lime_weights = np.exp(-image_squared_distance/(kernel_size**2))\n",
        "\n",
        "    return lime_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, obtain the blackbox model's predicted probabilities for a particular label of your choice."
      ],
      "metadata": {
        "id": "xrt_srXTBfHA"
      },
      "id": "xrt_srXTBfHA"
    },
    {
      "cell_type": "markdown",
      "id": "48d03f99-407f-4651-89f9-78b8c38ad253",
      "metadata": {
        "id": "48d03f99-407f-4651-89f9-78b8c38ad253"
      },
      "source": [
        "#### [Check-off 3] Lime Training Data for Image Models\n",
        "\n",
        "Implement the `generate_lime_training_data()` function below.\n",
        "\n",
        "- `original_label_index` is the index of the true label in the model's output.\n",
        "\n",
        "For example, if the model's output is `[0.6, 0.4]` for the probability 0.6 for \"cat\" and 0.4 for \"dog\", and the true label is \"cat\", then `original_label_index` is 0 and `predicted_probability` is 0.6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c253d3f-cd2e-4a60-8857-ce291b284fb4",
      "metadata": {
        "id": "4c253d3f-cd2e-4a60-8857-ce291b284fb4"
      },
      "outputs": [],
      "source": [
        "def generate_lime_training_data(original_image, fudged_image, segmentation,\n",
        "                                original_label_index, cnn_model,\n",
        "                                n_samples=400, initial_seed=None):\n",
        "    \"\"\"Returns the training data for the glassbox model that LIME uses to\n",
        "      interpret image models.\n",
        "    lime_X: Collection of segmentation vectors as the independent variable\n",
        "      for the glassbox model np.array in the shape (n_samples, n_segments)\n",
        "    lime_y: The CNN model's predicted probabilities on the true label from\n",
        "      the perturbed images np.array in the shape (n_samples,)\n",
        "    lime_w: The weights on each perturbed image from the function\n",
        "      `image_weight_kernel()`\n",
        "    \"\"\"\n",
        "    lime_X, lime_y, lime_w = [], [], []\n",
        "    rng = np.random.default_rng(seed=initial_seed)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        seed = rng.integers(2**63)\n",
        "        # TODO:\n",
        "\n",
        "        # perturbed_image, segmentation_vector = do something with perturb_image()\n",
        "        # distance_weight = do something with image_weight_kernel()\n",
        "        cnn_output = tf.squeeze(cnn_model(perturbed_image[np.newaxis, :]))\n",
        "        # predicted_probability = do something with cnn_output and original_label_index\n",
        "\n",
        "        lime_X += [segmentation_vector]\n",
        "        lime_y += [predicted_probability]\n",
        "        lime_w += [distance_weight]\n",
        "\n",
        "    lime_X, lime_y, lime_w = np.array(lime_X), np.array(lime_y), np.array(lime_w)\n",
        "\n",
        "    return lime_X, lime_y, lime_w"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fourth, in get_glassbox_image we train the glassbox model. Lastly, we can interpret the glassbox model instead of the blackbox model."
      ],
      "metadata": {
        "id": "7FtA-nP3CBvh"
      },
      "id": "7FtA-nP3CBvh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4822756b-26a9-4272-9795-856294efb00c",
      "metadata": {
        "id": "4822756b-26a9-4272-9795-856294efb00c"
      },
      "outputs": [],
      "source": [
        "def rgb2gray_3ch(original_image):\n",
        "    \"\"\"Desaturate the original image by repeating the gray channel three times\"\"\"\n",
        "    gray_image_1ch = rgb2gray(original_image)[:, :, np.newaxis]\n",
        "    gray_image_3ch = np.repeat(gray_image_1ch, repeats=3, axis=2)\n",
        "\n",
        "    return gray_image_3ch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd57809c-4088-49be-84b9-32465d3b012f",
      "metadata": {
        "id": "fd57809c-4088-49be-84b9-32465d3b012f"
      },
      "outputs": [],
      "source": [
        "def get_glassbox_image(original_image, fudged_image, segmentation, lime_X, lime_y, lime_w):\n",
        "    \"\"\"returns the glassbox image\n",
        "    in which the important part is in the original color\n",
        "    and the unimportant part is fudged and grayscaled\"\"\"\n",
        "\n",
        "    glassbox_lasso = Lasso(alpha=0.001).fit(lime_X, lime_y, sample_weight=lime_w) # this is step 4 where we train the glassbox model\n",
        "    important_seg_mask = (glassbox_lasso.coef_ > 0)\n",
        "    important_area_mask = important_seg_mask[segmentation]\n",
        "\n",
        "    glassbox_image = rgb2gray_3ch(fudged_image)\n",
        "    glassbox_image[important_area_mask] = original_image[important_area_mask]\n",
        "\n",
        "    return glassbox_image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a9445a8-cc99-4776-931c-a14b5050e2da",
      "metadata": {
        "id": "0a9445a8-cc99-4776-931c-a14b5050e2da"
      },
      "source": [
        "#### [Check-off 4] LIME on Image Models\n",
        "\n",
        "Now it's the time to put everything together and assemble the full LIME algorithm into the single function `lime_image()` below.\n",
        "- `segmentation`: First, you have to segment the original image with the function `safe_quickshift`. Make sure you input the correct default keyward arguments defined in the Python dictionary `seg_params`.\n",
        "- `fudged_image`: Second, you have to fudge the image with the function `fudge_image()`.\n",
        "- `lime_X`, `lime_y`, and `lime_w`: Then, you need the data to train the glassbox model. Use the function `generate_lime_training_data()` and input the correct default keyward arguments `n_samples` and `initial_seed`.\n",
        "- `glassbox_image`: Finally, create the glassbox image with the function `get_glassbox_image()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f096f2c8-3b23-4364-8602-a38408ff2761",
      "metadata": {
        "id": "f096f2c8-3b23-4364-8602-a38408ff2761"
      },
      "outputs": [],
      "source": [
        "def lime_image(original_image, original_label, label_dict, cnn_model,\n",
        "               seg_params=cifar_seg_params, n_samples=400, initial_seed=None):\n",
        "    \"\"\"returns the glassbox image, fudged image, and segmentation\"\"\"\n",
        "    original_label_index = label_dict[original_label]\n",
        "\n",
        "    # TODO:\n",
        "\n",
        "    # segmentation = do something with safe_quickshift()\n",
        "    # fudged_image = do something with fudge_image()\n",
        "    # lime_X, lime_y, lime_w = do something with generate_lime_training_data()\n",
        "    # glassbox_image = do something with get_glassbox_image()\n",
        "\n",
        "    return glassbox_image, fudged_image, segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f3cdc5b-4315-4060-b2aa-877d65c5bd5d",
      "metadata": {
        "id": "7f3cdc5b-4315-4060-b2aa-877d65c5bd5d"
      },
      "outputs": [],
      "source": [
        "def display_lime(original_image, fudged_image, segmentation,\n",
        "                 glassbox_image,\n",
        "                 mode=None):\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    ax[0].imshow(original_image)\n",
        "    ax[0].set_title(\"Orignal Image\")\n",
        "    ax[1].imshow(mark_boundaries(fudged_image, segmentation, mode=mode))\n",
        "    ax[1].set_title(\"Segmented and fudged\")\n",
        "    ax[2].imshow(glassbox_image)\n",
        "    ax[2].set_title(\"LIME\")\n",
        "    ax[0].set_axis_off()\n",
        "    ax[1].set_axis_off()\n",
        "    ax[2].set_axis_off()\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d2891d2-5240-4db6-b3da-6b988d8eab89",
      "metadata": {
        "id": "1d2891d2-5240-4db6-b3da-6b988d8eab89"
      },
      "source": [
        "After everything's done, you can finally inspect how the CNN model makes predictions on the CIFAR10 testing images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c211c751-8ea8-41d4-bc0b-6916feb0d716",
      "metadata": {
        "id": "c211c751-8ea8-41d4-bc0b-6916feb0d716"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# [25, 32, 43, 56, 219]\n",
        "for each_index in range(0, 20):\n",
        "    cifar_image = image_test[each_index]\n",
        "    cifar_label = label_test[each_index]\n",
        "    cifar_glass, cifar_fudged, cifar_seg = lime_image(cifar_image, cifar_label,\n",
        "                                                   cifar_class_dict, cnn_model)\n",
        "\n",
        "    cifar_predicted = cifar_class_list[tf.argmax(tf.squeeze(\n",
        "        cnn_model(cifar_image[np.newaxis, :])))]\n",
        "\n",
        "    fig_test, ax_test = display_lime(cifar_image, cifar_fudged, cifar_seg,\n",
        "                                     cifar_glass,\n",
        "                                     mode=\"subpixel\")\n",
        "    ax_test[0].set_title(f\"Actual: {cifar_label} / Pred: {cifar_predicted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A brief survey of other techniques\n",
        "\n",
        "[Write-Up] Select one of the following papers, and along with your completed notebook, submit a brief write-up of the main ideas from the paper and things that you found interesting about the approach.\n",
        "\n",
        "SHAP: https://arxiv.org/pdf/1705.07874\n",
        "\n",
        "Grad-CAM: https://arxiv.org/abs/1610.02391"
      ],
      "metadata": {
        "id": "tyezB2rJAVl2"
      },
      "id": "tyezB2rJAVl2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acknowledgements & Sources\n",
        "\n",
        "This lab is written by TA Yeunun Choo and edited by HTAs Vadim Kudlay and Nitya Thakkar\n",
        "\n",
        "- [LIME paper](https://arxiv.org/abs/1602.04938?context=cs)\n",
        "- [Python LIME package](https://github.com/marcotcr/lime)"
      ],
      "metadata": {
        "id": "DlIj7y0y7xzS"
      },
      "id": "DlIj7y0y7xzS"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}